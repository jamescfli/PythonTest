from __future__ import print_function

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Conv1D, MaxPooling1D
from keras.datasets import imdb

# Embedding
max_features = 20000
max_len = 100
embedding_size = 128

# Conv
kernel_size = 5
filters = 64
pool_size = 4

# LSTM
lstm_outupt_size = 70

# Train
batch_size = 30     # batch size is highly sensitive
epochs = 2

print('Loading data...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

print('Pad sequences (sample x time)')
x_train = sequence.pad_sequences(x_train, maxlen=max_len)
x_test = sequence.pad_sequences(x_test, maxlen=max_len)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

print('Build model...')
model = Sequential()
model.add(Embedding(max_features, embedding_size, input_length=max_len))
model.add(Dropout(0.25))
model.add(Conv1D(filters=filters,
                 kernel_size=kernel_size,
                 padding='valid',
                 activation='relu',
                 strides=1))
model.add(MaxPooling1D(pool_size=pool_size))
model.add(LSTM(lstm_outupt_size))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# embedding layer initial parameters
print(model.layers[0])
print(model.layers[0].get_weights()[0].shape)
print(model.layers[0].get_weights()[0][0, :])

print('Train...')
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(x_test, y_test))
# Epoch 2/7
# 25000/25000 [==============================] - 100s - loss: 0.1955 - acc: 0.9255 - val_loss: 0.3820 - val_acc: 0.8485
# .. 0.8485 > 0.8449 for bi-directional
# Epoch 7/7
# 25000/25000 [==============================] - 99s - loss: 0.0164 - acc: 0.9947 - val_loss: 0.8633 - val_acc: 0.8242
# .. 0.8242 < 0.8258 for bi-directional

score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

# embedding layer parameters after training
print(model.layers[0].get_weights()[0][0, :])

# Before:
# [-0.04945351 -0.01555111  0.00792786 -0.00631783  0.02398801  0.03383007
#  -0.01011925  0.03354141 -0.03999684 -0.01954479  0.02075784 -0.00176959
#  -0.02936645 -0.02132448 -0.01829553 -0.02445862  0.04353912  0.02832991
#  -0.00035477 -0.02244353 -0.00304531  0.02987088  0.00426165  0.02063164
#   0.0191722  -0.0005989   0.01286094  0.04530339  0.04986738 -0.01057338
#   0.0208518   0.02421054  0.0264335   0.03668426 -0.03670455 -0.04126099
#  -0.01458693  0.02388588 -0.04750942 -0.00238596  0.03185452 -0.02848732
#   0.02526085 -0.04988482  0.03093993  0.02679304 -0.0250216   0.0310371
#   0.04010553  0.02150823  0.02787875 -0.04261446 -0.03555858  0.04677944
#   0.04369599 -0.0289881  -0.01038376 -0.03696163 -0.0077896   0.02896208
#  -0.03969369 -0.0085113  -0.00039294  0.01111996 -0.01994814 -0.02944676
#  -0.03031474  0.02188108  0.0116442   0.03686157  0.0114862   0.0211177
#   0.04332211  0.01624903  0.04124181  0.03268442 -0.0448472   0.03061524
#   0.04116369 -0.00270156  0.03682068  0.02136732 -0.00134017  0.03069073
#   0.03696353  0.02355446 -0.02146204  0.03875155  0.04254497 -0.03006491
#   0.02449519 -0.00978311 -0.02915191 -0.02039336  0.00064512  0.0365178
#  -0.00433463  0.01159758  0.03732202  0.02374626  0.00607057  0.02365239
#   0.0061909   0.04905902  0.02985661 -0.02850922  0.00431116 -0.00237234
#   0.0246271  -0.00194431 -0.02710235  0.04575285  0.04501725  0.00929209
#   0.04020739  0.00312018  0.02039884  0.00810571 -0.01997007  0.04786913
#   0.00194262  0.04267457 -0.02019688  0.02637661  0.01738166  0.02262625
#   0.01555481 -0.0135954 ]

# After:
# [ -7.55082890e-02   2.70119108e-05   3.06178518e-02  -3.33303702e-03
#    1.79667734e-02   6.27670810e-02   2.61530876e-02   5.70584238e-02
#   -6.47603050e-02  -7.59578124e-02   7.80072361e-02  -6.54501468e-02
#   -2.82848105e-02   1.27501829e-04  -1.46599850e-02  -2.37597171e-02
#    4.40615863e-02   1.91572437e-03   4.98418659e-02  -7.63211325e-02
#   -5.72774559e-02   2.54100692e-02   7.79951399e-04  -4.37285146e-03
#    3.06566115e-02  -1.49378786e-02   2.08532773e-02   3.10771838e-02
#    5.92610911e-02   2.94815823e-02  -1.02655552e-02   4.24281470e-02
#    5.38285039e-02   3.25401351e-02  -7.85179213e-02  -3.93301956e-02
#   -5.96024096e-02   3.31691355e-02  -7.30415955e-02  -4.53065298e-02
#    3.16037126e-02   6.77597069e-04   3.06665283e-02  -4.12083752e-02
#    2.21633874e-02   3.19916606e-02  -4.55880575e-02   4.36101630e-02
#    1.63758341e-02   4.79840674e-02   5.58949485e-02  -4.02241983e-02
#   -4.14598100e-02   3.41766439e-02   3.91777791e-02   1.09541910e-02
#    1.29761863e-02  -7.36284256e-02  -3.06213815e-02   4.01190370e-02
#   -2.32054014e-03  -2.70149838e-02   4.03245911e-03   2.30011828e-02
#    8.77064001e-03  -6.25416189e-02  -1.13770058e-02   5.43937869e-02
#    4.96591926e-02   3.72880809e-02   3.75545695e-02   1.74894556e-02
#    5.30798212e-02   3.68090607e-02   3.37505378e-02   2.38236710e-02
#   -4.55308929e-02   5.26691899e-02   6.30145073e-02  -1.96237024e-02
#    6.76260069e-02   2.57024728e-02   1.04680881e-02   2.61304807e-02
#    5.91210239e-02   3.65351811e-02  -1.62352975e-02   3.47639546e-02
#    1.05046242e-01  -3.21698748e-02   1.33014079e-02  -1.36790946e-02
#   -5.63433841e-02  -3.14686857e-02   8.06945376e-03   4.91700284e-02
#   -5.24346419e-02   9.11232363e-03   6.93483874e-02   3.16738267e-03
#    1.03809815e-02   2.32902542e-02  -4.01085503e-02   4.86462712e-02
#   -9.12002008e-03  -8.09992924e-02   1.31776119e-02   1.66803841e-02
#    2.82317065e-02   1.34793837e-02  -8.84643290e-03   1.88986808e-02
#    5.04637361e-02   1.87515188e-02   7.18192235e-02   1.80975609e-02
#   -1.40640540e-02  -8.25050659e-03  -1.76669250e-03   6.87082633e-02
#   -9.95285809e-03   1.27464309e-01  -1.15196249e-02   8.83552954e-02
#    4.58155945e-02   3.83543558e-02   3.06039024e-02  -9.41748265e-03]

